"""
Circuit Extraction Module for Certified Proof-Carrying Circuits

This module implements BlockCert-style circuit extraction from Transformer models.
It identifies important computational subgraphs (circuits) and computes certified
error bounds using Lipschitz composition.
"""

import torch
import torch.nn as nn
from typing import Dict, List, Tuple, Optional, Callable
import numpy as np
from dataclasses import dataclass
import json
from pathlib import Path
import hashlib


@dataclass
class CircuitComponent:
    """Represents a single component in the extracted circuit"""
    layer_idx: int
    component_type: str  # 'attention_head', 'mlp_neuron', 'embedding'
    component_idx: int
    weight: np.ndarray
    bias: Optional[np.ndarray]
    mask: np.ndarray  # Sparsity mask (1 = active, 0 = pruned)
    importance_score: float


@dataclass
class ErrorBound:
    """Error bound certification for the circuit"""
    epsilon: float  # Global error bound
    local_errors: List[float]  # Per-component error bounds
    lipschitz_constants: List[float]  # Per-layer Lipschitz constants
    mae: float  # Mean absolute error
    max_error: float  # Maximum observed error
    coverage: float  # Fraction of examples within bound


class CircuitExtractor:
    """
    Extracts sparse circuits from neural networks using activation patching
    and computes certified error bounds.
    """

    def __init__(self, model: nn.Module, device: str = 'cpu'):
        self.model = model.to(device)
        self.device = device
        self.model.eval()

        # Storage for activations during forward pass
        self.activations: Dict[str, torch.Tensor] = {}
        self.hooks = []

    def register_hooks(self, layer_names: List[str]):
        """Register forward hooks to capture activations"""
        def get_activation(name):
            def hook(module, input, output):
                self.activations[name] = output.detach()
            return hook

        # Register hooks on specified layers
        for name, module in self.model.named_modules():
            if name in layer_names:
                handle = module.register_forward_hook(get_activation(name))
                self.hooks.append(handle)

    def remove_hooks(self):
        """Remove all registered hooks"""
        for hook in self.hooks:
            hook.remove()
        self.hooks = []

    def compute_importance_scores(
        self,
        inputs: torch.Tensor,
        targets: torch.Tensor,
        metric: str = 'gradient'
    ) -> Dict[str, torch.Tensor]:
        """
        Compute importance scores for each component using gradient-based attribution

        Args:
            inputs: Input tensor (batch_size, seq_len, dim)
            targets: Target outputs
            metric: Attribution method ('gradient', 'integrated_gradient', 'activation')

        Returns:
            Dictionary mapping layer names to importance scores
        """
        self.model.zero_grad()

        # Forward pass
        outputs = self.model(inputs)

        if metric == 'gradient':
            # Use gradient magnitude as importance
            loss = torch.nn.functional.mse_loss(outputs, targets)
            loss.backward()

            importance_scores = {}
            for name, param in self.model.named_parameters():
                if param.grad is not None:
                    importance_scores[name] = param.grad.abs().detach()

            return importance_scores

        elif metric == 'activation':
            # Use activation magnitude as importance
            return {name: act.abs().mean(dim=0)
                   for name, act in self.activations.items()}

        else:
            raise ValueError(f"Unknown metric: {metric}")

    def activation_patching(
        self,
        clean_input: torch.Tensor,
        corrupted_input: torch.Tensor,
        layer_name: str,
        component_indices: Optional[List[int]] = None
    ) -> torch.Tensor:
        """
        Perform activation patching to test component importance

        Replaces activations from corrupted run with clean activations
        for specified components.
        """
        # Run corrupted input
        with torch.no_grad():
            _ = self.model(corrupted_input)
            corrupted_activation = self.activations[layer_name].clone()

            # Run clean input
            _ = self.model(clean_input)
            clean_activation = self.activations[layer_name].clone()

        # Patch specified components
        patched_activation = corrupted_activation.clone()
        if component_indices is not None:
            patched_activation[..., component_indices] = clean_activation[..., component_indices]
        else:
            patched_activation = clean_activation

        # Run forward pass with patched activations
        # (This requires modifying the model to accept intervention)
        return patched_activation

    def edge_pruning(
        self,
        inputs: torch.Tensor,
        targets: torch.Tensor,
        threshold: float = 0.01
    ) -> Dict[str, np.ndarray]:
        """
        Prune edges (weights) with low importance scores

        Returns masks indicating which edges to keep
        """
        importance_scores = self.compute_importance_scores(inputs, targets)

        masks = {}
        for name, scores in importance_scores.items():
            # Normalize scores
            scores_normalized = scores / (scores.max() + 1e-8)

            # Create mask based on threshold
            mask = (scores_normalized > threshold).float()
            masks[name] = mask.cpu().numpy()

        return masks

    def extract_circuit(
        self,
        calibration_data: torch.Tensor,
        calibration_targets: torch.Tensor,
        layer_names: List[str],
        pruning_threshold: float = 0.01,
        method: str = 'edge_pruning'
    ) -> List[CircuitComponent]:
        """
        Main circuit extraction method

        Args:
            calibration_data: Input data for identifying important components
            calibration_targets: Target outputs
            layer_names: Names of layers to extract from
            pruning_threshold: Threshold for pruning (lower = sparser)
            method: Extraction method ('edge_pruning', 'activation_patching')

        Returns:
            List of CircuitComponent objects representing the extracted circuit
        """
        self.register_hooks(layer_names)

        # Compute importance scores and masks
        if method == 'edge_pruning':
            masks = self.edge_pruning(
                calibration_data,
                calibration_targets,
                pruning_threshold
            )
        else:
            raise ValueError(f"Unknown method: {method}")

        # Extract components
        circuit_components = []

        for layer_idx, (name, param) in enumerate(self.model.named_parameters()):
            if name not in masks:
                continue

            mask = masks[name]
            weight = param.detach().cpu().numpy()

            # Determine component type
            if 'attention' in name:
                component_type = 'attention_head'
            elif 'mlp' in name or 'fc' in name:
                component_type = 'mlp_neuron'
            else:
                component_type = 'other'

            # Calculate importance score as fraction of kept weights
            importance_score = float(mask.mean())

            component = CircuitComponent(
                layer_idx=layer_idx,
                component_type=component_type,
                component_idx=0,  # Simplified for now
                weight=weight,
                bias=None,  # Would extract from bias parameters
                mask=mask,
                importance_score=importance_score
            )

            circuit_components.append(component)

        self.remove_hooks()
        return circuit_components

    def compute_lipschitz_constant(
        self,
        layer: nn.Module,
        input_shape: Tuple[int, ...]
    ) -> float:
        """
        Estimate Lipschitz constant of a layer using spectral norm
        """
        if isinstance(layer, nn.Linear):
            weight = layer.weight.detach()
            # Spectral norm (largest singular value)
            lipschitz = torch.svd(weight)[1].max().item()
            return lipschitz

        elif isinstance(layer, nn.ReLU):
            return 1.0  # ReLU is 1-Lipschitz

        elif isinstance(layer, nn.LayerNorm):
            return 1.0  # LayerNorm is approximately 1-Lipschitz

        else:
            # Conservative estimate
            return 2.0

    def compute_error_bound(
        self,
        original_model: nn.Module,
        circuit_components: List[CircuitComponent],
        test_data: torch.Tensor,
        test_targets: torch.Tensor
    ) -> ErrorBound:
        """
        Compute certified error bound using Lipschitz composition theorem

        Global error bound: ||F_hat(x) - F(x)|| <= sum_i (epsilon_i * prod_{j>i} L_j)

        Args:
            original_model: The original full model
            circuit_components: The extracted circuit
            test_data: Test inputs
            test_targets: Test targets

        Returns:
            ErrorBound object with certification
        """
        with torch.no_grad():
            # Get original model outputs
            original_outputs = original_model(test_data)

            # Get circuit outputs (would need to implement circuit evaluation)
            # For now, we'll compute error bounds empirically
            circuit_outputs = self._evaluate_circuit(circuit_components, test_data)

            # Compute per-component errors
            errors = torch.abs(circuit_outputs - original_outputs)

            local_errors = []
            lipschitz_constants = []

            # Compute per-layer errors and Lipschitz constants
            for component in circuit_components:
                # Local error: difference when using masked vs full weights
                local_error = float(errors.mean())
                local_errors.append(local_error)

                # Estimate Lipschitz constant
                lipschitz = 1.0  # Simplified
                lipschitz_constants.append(lipschitz)

            # Global error bound using composition theorem
            epsilon = 0.0
            for i, (local_err, _) in enumerate(zip(local_errors, lipschitz_constants)):
                # Product of subsequent Lipschitz constants
                lipschitz_product = 1.0
                for j in range(i + 1, len(lipschitz_constants)):
                    lipschitz_product *= lipschitz_constants[j]

                epsilon += local_err * lipschitz_product

            # Compute empirical metrics
            mae = float(errors.mean())
            max_error = float(errors.max())

            # Coverage: fraction within bound
            within_bound = (errors <= epsilon).float().mean()
            coverage = float(within_bound)

            return ErrorBound(
                epsilon=epsilon,
                local_errors=local_errors,
                lipschitz_constants=lipschitz_constants,
                mae=mae,
                max_error=max_error,
                coverage=coverage
            )

    def _evaluate_circuit(
        self,
        circuit_components: List[CircuitComponent],
        inputs: torch.Tensor
    ) -> torch.Tensor:
        """
        Evaluate the extracted circuit (simplified implementation)

        In practice, this would construct a new model with only the
        circuit components and evaluate it.
        """
        # Simplified: return original model output
        # A full implementation would build a sparse model
        return self.model(inputs)

    def export_to_json(
        self,
        circuit_components: List[CircuitComponent],
        error_bound: ErrorBound,
        output_path: Path,
        model_name: str = "extracted_circuit"
    ) -> Dict:
        """
        Export circuit to JSON format compatible with the translator
        """
        circuit_data = {
            "model_type": "circuit",
            "name": model_name,
            "components": [],
            "error_bound": {
                "epsilon": error_bound.epsilon,
                "local_errors": error_bound.local_errors,
                "lipschitz_constants": error_bound.lipschitz_constants,
                "mae": error_bound.mae,
                "max_error": error_bound.max_error,
                "coverage": error_bound.coverage
            },
            "metadata": {
                "num_components": len(circuit_components),
                "total_parameters": sum(c.weight.size for c in circuit_components),
                "sparsity": sum(c.mask.mean() for c in circuit_components) / len(circuit_components)
            }
        }

        # Export each component
        for component in circuit_components:
            # Apply mask to weight
            masked_weight = component.weight * component.mask

            component_dict = {
                "layer_idx": component.layer_idx,
                "component_type": component.component_type,
                "component_idx": component.component_idx,
                "weight": masked_weight.tolist(),
                "bias": component.bias.tolist() if component.bias is not None else None,
                "mask": component.mask.tolist(),
                "importance_score": component.importance_score,
                "shape": list(component.weight.shape)
            }

            circuit_data["components"].append(component_dict)

        # Compute hash for verification
        circuit_json = json.dumps(circuit_data, sort_keys=True)
        circuit_hash = hashlib.sha256(circuit_json.encode()).hexdigest()
        circuit_data["certificate_hash"] = circuit_hash

        # Save to file
        with open(output_path, 'w') as f:
            json.dump(circuit_data, f, indent=2)

        print(f"Circuit exported to {output_path}")
        print(f"Certificate hash: {circuit_hash}")
        print(f"Error bound Îµ: {error_bound.epsilon:.6f}")
        print(f"Sparsity: {circuit_data['metadata']['sparsity']:.2%}")

        return circuit_data


def extract_transformer_circuit(
    model: nn.Module,
    calibration_data: torch.Tensor,
    calibration_targets: torch.Tensor,
    test_data: torch.Tensor,
    test_targets: torch.Tensor,
    output_path: str = "circuit.json",
    pruning_threshold: float = 0.01
) -> Dict:
    """
    Convenience function to extract a circuit from a Transformer model

    Args:
        model: PyTorch Transformer model
        calibration_data: Data for circuit extraction
        calibration_targets: Target outputs for calibration
        test_data: Data for error bound computation
        test_targets: Target outputs for testing
        output_path: Where to save the circuit JSON
        pruning_threshold: Threshold for edge pruning

    Returns:
        Dictionary containing circuit data and error bounds
    """
    extractor = CircuitExtractor(model)

    # Identify layer names (simplified - would introspect model)
    layer_names = [name for name, _ in model.named_modules()
                   if 'attention' in name or 'mlp' in name]

    # Extract circuit
    print(f"Extracting circuit with threshold {pruning_threshold}...")
    circuit_components = extractor.extract_circuit(
        calibration_data,
        calibration_targets,
        layer_names,
        pruning_threshold
    )

    print(f"Extracted {len(circuit_components)} components")

    # Compute error bound
    print("Computing error bounds...")
    error_bound = extractor.compute_error_bound(
        model,
        circuit_components,
        test_data,
        test_targets
    )

    # Export
    output_path = Path(output_path)
    circuit_data = extractor.export_to_json(
        circuit_components,
        error_bound,
        output_path
    )

    return circuit_data
