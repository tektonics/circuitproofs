#!/usr/bin/env python3
"""
Enterprise features test suite for the FormalVerifML framework.
This script tests all the new enterprise features including:
- Large-scale models (100M+ parameters)
- Vision Transformers and other architectures
- Distributed verification
- Enterprise features (multi-user, audit logging, security)
"""

import subprocess
import sys
import json
import time
import os
import argparse
from pathlib import Path
from typing import Dict, List, Any
import requests


class EnterpriseFeatureTester:
    """Test suite for enterprise features."""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.results = {}
        self.start_time = time.time()

    def test_large_scale_models(self) -> Dict[str, Any]:
        """Test large-scale model capabilities."""
        print("=" * 60)
        print("TESTING LARGE-SCALE MODELS (100M+ PARAMETERS)")
        print("=" * 60)

        try:
            # Test with large HuggingFace models
            large_models = [
                "microsoft/DialoGPT-medium",  # ~345M parameters
                "gpt2-medium",  # ~355M parameters
                "bert-large-uncased",  # ~340M parameters
            ]

            results = {}
            for model_name in large_models:
                print(f"Testing large model: {model_name}")

                try:
                    # Test model loading and basic inference
                    cmd = [
                        sys.executable,
                        "-c",
                        f"""
import torch
from transformers import AutoModel, AutoTokenizer
import time
import psutil

print(f"Loading {model_name}...")
start_time = time.time()

# Load model
model = AutoModel.from_pretrained("{model_name}")
tokenizer = AutoTokenizer.from_pretrained("{model_name}")

load_time = time.time() - start_time
param_count = sum(p.numel() for p in model.parameters())
memory_mb = psutil.Process().memory_info().rss / 1024 / 1024

print(f"Model loaded in {{load_time:.2f}}s")
print(f"Parameters: {{param_count:,}}")
print(f"Memory usage: {{memory_mb:.1f}}MB")

# Test inference
text = "This is a test of the large-scale model capabilities."
inputs = tokenizer(text, return_tensors="pt", max_length=512, truncation=True, padding=True)

with torch.no_grad():
    outputs = model(**inputs)

print(f"Output shape: {{outputs.last_hidden_state.shape}}")
print("Large model test completed successfully")
""",
                    ]

                    result = subprocess.run(
                        cmd, capture_output=True, text=True, timeout=600
                    )

                    if result.returncode == 0:
                        results[model_name] = {
                            "status": "success",
                            "output": result.stdout,
                            "error": None,
                        }
                    else:
                        results[model_name] = {
                            "status": "error",
                            "output": result.stdout,
                            "error": result.stderr,
                        }

                except subprocess.TimeoutExpired:
                    results[model_name] = {
                        "status": "timeout",
                        "output": None,
                        "error": "Model loading timed out",
                    }
                except Exception as e:
                    results[model_name] = {
                        "status": "error",
                        "output": None,
                        "error": str(e),
                    }

            # Save results
            with open("test_results_large_models.json", "w") as f:
                json.dump(results, f, indent=2)

            print("âœ… Large-scale model tests completed")
            return {"status": "success", "results": results}

        except Exception as e:
            print(f"âŒ Large-scale model tests failed: {str(e)}")
            return {"status": "error", "error": str(e)}

    def test_vision_models(self) -> Dict[str, Any]:
        """Test Vision Transformer and other vision models."""
        print("=" * 60)
        print("TESTING VISION TRANSFORMERS")
        print("=" * 60)

        try:
            # Test Vision Transformer models
            vision_models = [
                "google/vit-base-patch16-224",  # Vision Transformer
                "microsoft/swin-base-patch4-window7-224",  # Swin Transformer
                "openai/clip-vit-base-patch32",  # CLIP (multi-modal)
            ]

            results = {}
            for model_name in vision_models:
                print(f"Testing vision model: {model_name}")

                try:
                    cmd = [
                        sys.executable,
                        "-c",
                        f"""
import torch
from transformers import AutoModel, AutoProcessor, AutoImageProcessor
from PIL import Image
import numpy as np
import time
import psutil
import os

print(f"Loading vision model: {model_name}...")
start_time = time.time()

# Load model and processor
model = AutoModel.from_pretrained("{model_name}")

if "clip" in "{model_name}".lower():
    processor = AutoProcessor.from_pretrained("{model_name}")
else:
    processor = AutoImageProcessor.from_pretrained("{model_name}")

load_time = time.time() - start_time
param_count = sum(p.numel() for p in model.parameters())
memory_mb = psutil.Process().memory_info().rss / 1024 / 1024

print(f"Vision model loaded in {{load_time:.2f}}s")
print(f"Parameters: {{param_count:,}}")
print(f"Memory usage: {{memory_mb:.1f}}MB")

# Use test asset image if available, otherwise create a dummy image
test_image_path = "translator/test_assets/synthetic_pattern.png"
if os.path.exists(test_image_path):
    dummy_image = Image.open(test_image_path).convert("RGB")
    print(f"Loaded test image: {{test_image_path}}, size={{dummy_image.size}}")
else:
    # Fallback: create a proper dummy image using PIL (RGB, 224x224)
    dummy_array = np.random.randint(0, 256, (224, 224, 3), dtype=np.uint8)
    dummy_image = Image.fromarray(dummy_array, mode='RGB')
    print(f"Created dummy PIL image: {{dummy_image.size}}, mode={{dummy_image.mode}}")

# Test inference
with torch.no_grad():
    if "clip" in "{model_name}".lower():
        # CLIP expects both image and text
        text = ["a photo of a cat", "a photo of a dog"]
        inputs = processor(images=dummy_image, text=text, return_tensors="pt", padding=True)
        outputs = model(**inputs)
    else:
        # Vision Transformer expects only image
        inputs = processor(images=dummy_image, return_tensors="pt")
        outputs = model(**inputs)

# Handle different model output structures
if hasattr(outputs, 'last_hidden_state'):
    print(f"Output shape: {{outputs.last_hidden_state.shape}}")
elif hasattr(outputs, 'image_embeds'):
    # CLIP model returns image_embeds and text_embeds
    print(f"Image embeds shape: {{outputs.image_embeds.shape}}")
    print(f"Text embeds shape: {{outputs.text_embeds.shape}}")
elif hasattr(outputs, 'logits'):
    print(f"Logits shape: {{outputs.logits.shape}}")
else:
    print(f"Output keys: {{list(outputs.keys())}}")
print("Vision model test completed successfully")
""",
                    ]

                    result = subprocess.run(
                        cmd, capture_output=True, text=True, timeout=300
                    )

                    if result.returncode == 0:
                        results[model_name] = {
                            "status": "success",
                            "output": result.stdout,
                            "error": None,
                        }
                    else:
                        results[model_name] = {
                            "status": "error",
                            "output": result.stdout,
                            "error": result.stderr,
                        }

                except subprocess.TimeoutExpired:
                    results[model_name] = {
                        "status": "timeout",
                        "output": None,
                        "error": "Vision model loading timed out",
                    }
                except Exception as e:
                    results[model_name] = {
                        "status": "error",
                        "output": None,
                        "error": str(e),
                    }

            # Save results
            with open("test_results_vision_models.json", "w") as f:
                json.dump(results, f, indent=2)

            print("âœ… Vision model tests completed")
            return {"status": "success", "results": results}

        except Exception as e:
            print(f"âŒ Vision model tests failed: {str(e)}")
            return {"status": "error", "error": str(e)}

    def test_distributed_verification(self) -> Dict[str, Any]:
        """Test distributed verification capabilities."""
        print("=" * 60)
        print("TESTING DISTRIBUTED VERIFICATION")
        print("=" * 60)

        try:
            # Test Lean build with distributed verification
            cmd = ["lake", "build"]
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=180)

            if result.returncode == 0:
                print("âœ… Distributed verification Lean build completed")
                return {"status": "success", "output": result.stdout}
            else:
                print(f"âŒ Distributed verification Lean build failed: {result.stderr}")
                return {"status": "error", "error": result.stderr}

        except subprocess.TimeoutExpired:
            print("âŒ Distributed verification tests timed out")
            return {"status": "timeout"}
        except Exception as e:
            print(f"âŒ Distributed verification tests error: {str(e)}")
            return {"status": "error", "error": str(e)}

    def test_enterprise_features(self) -> Dict[str, Any]:
        """Test enterprise features (multi-user, audit logging, security)."""
        print("=" * 60)
        print("TESTING ENTERPRISE FEATURES")
        print("=" * 60)

        try:
            # Test web interface with enterprise features
            web_process = subprocess.Popen(
                [sys.executable, "webapp/app.py"],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
            )

            # Wait for server to start
            time.sleep(5)

            # Test enterprise endpoints
            test_results = {}

            # Test basic connectivity
            try:
                response = requests.get("http://localhost:5000", timeout=10)
                test_results["basic_connectivity"] = {
                    "status": "success" if response.status_code == 200 else "error",
                    "status_code": response.status_code,
                }
            except requests.RequestException as e:
                test_results["basic_connectivity"] = {
                    "status": "error",
                    "error": str(e),
                }

            # Test model visualization
            try:
                response = requests.get("http://localhost:5000/visualize", timeout=10)
                test_results["model_visualization"] = {
                    "status": "success" if response.status_code == 200 else "error",
                    "status_code": response.status_code,
                }
            except requests.RequestException as e:
                test_results["model_visualization"] = {
                    "status": "error",
                    "error": str(e),
                }

            # Test logs endpoint
            try:
                response = requests.get("http://localhost:5000/logs", timeout=10)
                test_results["logs_endpoint"] = {
                    "status": "success" if response.status_code == 200 else "error",
                    "status_code": response.status_code,
                }
            except requests.RequestException as e:
                test_results["logs_endpoint"] = {"status": "error", "error": str(e)}

            # Cleanup
            web_process.terminate()
            web_process.wait()

            # Save results
            with open("test_results_enterprise.json", "w") as f:
                json.dump(test_results, f, indent=2)

            print("âœ… Enterprise features tests completed")
            return {"status": "success", "results": test_results}

        except Exception as e:
            print(f"âŒ Enterprise features tests failed: {str(e)}")
            return {"status": "error", "error": str(e)}

    def test_memory_optimization(self) -> Dict[str, Any]:
        """Test memory optimization for large models."""
        print("=" * 60)
        print("TESTING MEMORY OPTIMIZATION")
        print("=" * 60)

        try:
            # Test memory-optimized model generation
            cmd = [
                sys.executable,
                "translator/generate_lean_model.py",
                "--model_json",
                "translator/sample_transformer.json",
                "--output_lean",
                "lean/FormalVerifML/generated/memory_optimized_enterprise.lean",
            ]

            result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)

            if result.returncode == 0:
                print("âœ… Memory optimization tests completed successfully")
                return {"status": "success"}
            else:
                print(f"âŒ Memory optimization tests failed: {result.stderr}")
                return {"status": "error", "error": result.stderr}

        except subprocess.TimeoutExpired:
            print("âŒ Memory optimization tests timed out")
            return {"status": "timeout"}
        except Exception as e:
            print(f"âŒ Memory optimization tests error: {str(e)}")
            return {"status": "error", "error": str(e)}

    def test_security_features(self) -> Dict[str, Any]:
        """Test security features (encryption, rate limiting, etc.)."""
        print("=" * 60)
        print("TESTING SECURITY FEATURES")
        print("=" * 60)

        try:
            # Test encryption/decryption
            test_data = "sensitive_model_data"

            cmd = [
                sys.executable,
                "-c",
                f"""
# Simulate encryption/decryption
test_data = "{test_data}"
encrypted = f"encrypted_{{test_data}}"
decrypted = encrypted.replace("encrypted_", "")

print(f"Original: {{test_data}}")
print(f"Encrypted: {{encrypted}}")
print(f"Decrypted: {{decrypted}}")
print("Security test completed successfully")
""",
            ]

            result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)

            if result.returncode == 0:
                print("âœ… Security features tests completed successfully")
                return {"status": "success", "output": result.stdout}
            else:
                print(f"âŒ Security features tests failed: {result.stderr}")
                return {"status": "error", "error": result.stderr}

        except Exception as e:
            print(f"âŒ Security features tests error: {str(e)}")
            return {"status": "error", "error": str(e)}

    def run_all_enterprise_tests(self) -> Dict[str, Any]:
        """Run all enterprise feature tests."""
        print("STARTING ENTERPRISE FEATURE TEST SUITE")
        print("=" * 60)

        # Run all test categories
        test_categories = [
            ("large_scale_models", self.test_large_scale_models),
            ("vision_models", self.test_vision_models),
            ("distributed_verification", self.test_distributed_verification),
            ("enterprise_features", self.test_enterprise_features),
            ("memory_optimization", self.test_memory_optimization),
            ("security_features", self.test_security_features),
        ]

        for category, test_func in test_categories:
            if self.config.get(f"run_{category}", True):
                self.results[category] = test_func()
            else:
                print(f"Skipping {category} (disabled in config)")
                self.results[category] = {"status": "skipped"}

        return self.results

    def generate_enterprise_report(self) -> str:
        """Generate enterprise feature test report."""
        total_time = time.time() - self.start_time

        # Count results
        total_tests = len(self.results)
        successful_tests = sum(
            1 for r in self.results.values() if r.get("status") == "success"
        )
        failed_tests = sum(
            1 for r in self.results.values() if r.get("status") == "error"
        )
        timeout_tests = sum(
            1 for r in self.results.values() if r.get("status") == "timeout"
        )
        skipped_tests = sum(
            1 for r in self.results.values() if r.get("status") == "skipped"
        )

        success_rate = (successful_tests / total_tests * 100) if total_tests > 0 else 0

        report = f"""
ENTERPRISE FEATURE TEST REPORT
{'='*60}
Total Test Categories: {total_tests}
Successful: {successful_tests}
Failed: {failed_tests}
Timeout: {timeout_tests}
Skipped: {skipped_tests}
Total Execution Time: {total_time:.2f}s
Success Rate: {success_rate:.1f}%

ENTERPRISE FEATURES SUMMARY:
{'='*60}
âœ… Large-Scale Models (100M+ parameters): {'âœ“' if self.results.get('large_scale_models', {}).get('status') == 'success' else 'âœ—'}
âœ… Vision Transformers (ViT, Swin, CLIP): {'âœ“' if self.results.get('vision_models', {}).get('status') == 'success' else 'âœ—'}
âœ… Distributed Verification: {'âœ“' if self.results.get('distributed_verification', {}).get('status') == 'success' else 'âœ—'}
âœ… Enterprise Features (Multi-user, Audit): {'âœ“' if self.results.get('enterprise_features', {}).get('status') == 'success' else 'âœ—'}
âœ… Memory Optimization: {'âœ“' if self.results.get('memory_optimization', {}).get('status') == 'success' else 'âœ—'}
âœ… Security Features: {'âœ“' if self.results.get('security_features', {}).get('status') == 'success' else 'âœ—'}

DETAILED RESULTS:
{'='*60}
"""

        for category, result in self.results.items():
            status = result.get("status", "unknown")
            report += f"\n{category.upper()}:\n"
            report += f"  Status: {status}\n"

            if "error" in result:
                report += f"  Error: {result['error']}\n"
            if "output" in result:
                report += f"  Output: {result['output'][:200]}...\n"
            if "results" in result:
                report += f"  Results: {len(result['results'])} items tested\n"

        return report


def main():
    parser = argparse.ArgumentParser(
        description="Test enterprise features for FormalVerifML"
    )
    parser.add_argument(
        "--config",
        type=str,
        default="enterprise_test_config.json",
        help="Path to enterprise test configuration file",
    )
    parser.add_argument(
        "--output",
        type=str,
        default="enterprise_test_report.txt",
        help="Path to output report file",
    )

    args = parser.parse_args()

    # Default enterprise configuration
    config = {
        "run_large_scale_models": True,
        "run_vision_models": True,
        "run_distributed_verification": True,
        "run_enterprise_features": True,
        "run_memory_optimization": True,
        "run_security_features": True,
    }

    # Load custom config if provided
    if os.path.exists(args.config):
        with open(args.config, "r") as f:
            custom_config = json.load(f)
            config.update(custom_config)

    # Run tests
    tester = EnterpriseFeatureTester(config)
    results = tester.run_all_enterprise_tests()

    # Generate and save report
    report = tester.generate_enterprise_report()

    with open(args.output, "w") as f:
        f.write(report)

    print("\n" + report)
    print(f"\nDetailed report saved to: {args.output}")

    # Exit with appropriate code
    successful_tests = sum(1 for r in results.values() if r.get("status") == "success")
    total_tests = len(results)

    if successful_tests == total_tests:
        print("ğŸ‰ All enterprise feature tests passed!")
        exit(0)
    else:
        print(f"âš ï¸  {total_tests - successful_tests} enterprise test(s) failed")
        exit(1)


if __name__ == "__main__":
    main()
